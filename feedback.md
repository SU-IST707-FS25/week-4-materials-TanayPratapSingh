# Assignment Feedback: Week 4: Dimensionality Reduction

**Student:** TanayPratapSingh
**Total Score:** 26/40 (65.0%)

**Grade Category:** D (Poor)

---

## Problem Breakdown

### Exercise 1 (6/16 = 37.5%)

**Part pipeline-part1** (pipeline-part1.code): 0/0 points

_Feedback:_ Good start: you fit PCA to 2 components and visualized the 2D embedding colored by labels. To fully match “visualize the approximation,” also inverse_transform to reconstruct digits and display them (e.g., original vs. reconstructed). Optionally report explained variance.

**Part pipeline-part2** (pipeline-part2.code): 1/4 points

_Feedback:_ You fit PCA and plotted a scree plot, but the task was to reduce to 2 components and show a 2D scatter colored by class. Missing n_components=2, transform to 2D, and the class-colored scatter. Reuse your prior code that did this. Partial credit for correct PCA usage.

**Part pipeline-part3** (pipeline-part3.code): 1/4 points

_Feedback:_ You computed cumulative variance and the 95% threshold using pca_full.explained_variance_ratio_, which is correct, but you did not create the required scree plot. Plot the first 40 components’ variance explained (ideally as percent) vs component index with proper labels.

**Part pipeline-part4** (pipeline-part4.code): 3/4 points

_Feedback:_ You correctly leveraged n_components_95 from prior work to select components, which fulfills the step’s goal. However, this cell doesn’t compute or report the number of components, and uses undefined digit_reconstructed. Add a print of n_components_95 and remove/fix the undefined

**Part pipeline-part5** (pipeline-part5.code): 1/4 points

_Feedback:_ This doesn’t address Step 5. You ran KNN and PCA(0.80) on the dataset, but you didn’t visualize a digit. Use your n_components_95, transform a digit, inverse_transform it, and call plot_mnist_digit on the reconstruction (ideally the same digit as earlier).

---

### Exercise 2 (10/10 = 100.0%)

**Part ex1-part1** (ex1-part1.code): 4/4 points

_Feedback:_ Well done. You correctly applied t-SNE, sensibly used a 2k subset for speed, set key params (perplexity, random_state), and produced a clear 2D scatter with labels and colorbar. This meets the goal and should work as intended.

**Part ex1-part2** (ex1-part2.code): 3/3 points

_Feedback:_ Good job training KNN on your t-SNE embedding and reporting accuracy. Note: you evaluated on the training set, so the score may be optimistic. For a fairer assessment, use a train/test split or cross-validation on X_tsne.

**Part ex1-part3** (ex1-part3.code): 3/3 points

_Feedback:_ Correct: you trained KNN on the UMAP embedding and computed accuracy, mirroring your prior t-SNE approach. This fulfills the task using your existing variables (X_umap, y_subset). Nice work.

---

### Exercise 4 (10/14 = 71.4%)

**Part ex2-part1** (ex2-part1.code): 0/0 points

_Feedback:_ Good start: you apply PCA (2/3 dims), train KNN, and plot 2D. However, you only evaluate on training data—use a train/test split on reduced features. You didn’t try UMAP or vary its parameters, and no k sweep or comparative visuals. Expand to meet the prompt.

**Part ex2-part2** (ex2-part2.code): 3/7 points

_Feedback:_ You implemented dimensionality reduction + KNN and visualization, but you used UMAP instead of PCA, which does not meet the “Try with PCA” requirement nor align with your prior PCA-based workflow. Replace UMAP with PCA (fit_transform/transform) and report results. Ignore the plac

**Part ex2-part3** (ex2-part3.answer): 7/7 points

_Feedback:_ Strong, accurate explanation. You correctly contrast PCA vs UMAP, interpret n_neighbors effects for KNN, and note 2D vs 3D adds little. The trends you describe match typical outcomes and align with your prior UMAP setup. Nice job connecting method behavior to accuracy.

---

## Additional Information

This feedback was automatically generated by the autograder using LLM-based evaluation.

**Generated:** 2025-10-27 18:51:19 UTC

If you have questions about your grade, please reach out to the instructor.

---

*Powered by [Grade-Lite](https://github.com/your-repo/grade-lite) Autograder*